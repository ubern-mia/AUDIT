{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AUDIT library","text":"<p>For full documentation visit AUDIT repository.</p>"},{"location":"#heading","title":"Heading","text":""},{"location":"#heading_1","title":"Heading","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"about/LICENSE/","title":"License","text":"<pre><code>                             Apache License\n                       Version 2.0, January 2004\n                    http://www.apache.org/licenses/\n</code></pre> <p>TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION</p> <ol> <li> <p>Definitions.</p> <p>\"License\" shall mean the terms and conditions for use, reproduction,   and distribution as defined by Sections 1 through 9 of this document.</p> <p>\"Licensor\" shall mean the copyright owner or entity authorized by   the copyright owner that is granting the License.</p> <p>\"Legal Entity\" shall mean the union of the acting entity and all   other entities that control, are controlled by, or are under common   control with that entity. For the purposes of this definition,   \"control\" means (i) the power, direct or indirect, to cause the   direction or management of such entity, whether by contract or   otherwise, or (ii) ownership of fifty percent (50%) or more of the   outstanding shares, or (iii) beneficial ownership of such entity.</p> <p>\"You\" (or \"Your\") shall mean an individual or Legal Entity   exercising permissions granted by this License.</p> <p>\"Source\" form shall mean the preferred form for making modifications,   including but not limited to software source code, documentation   source, and configuration files.</p> <p>\"Object\" form shall mean any form resulting from mechanical   transformation or translation of a Source form, including but   not limited to compiled object code, generated documentation,   and conversions to other media types.</p> <p>\"Work\" shall mean the work of authorship, whether in Source or   Object form, made available under the License, as indicated by a   copyright notice that is included in or attached to the work   (an example is provided in the Appendix below).</p> <p>\"Derivative Works\" shall mean any work, whether in Source or Object   form, that is based on (or derived from) the Work and for which the   editorial revisions, annotations, elaborations, or other modifications   represent, as a whole, an original work of authorship. For the purposes   of this License, Derivative Works shall not include works that remain   separable from, or merely link (or bind by name) to the interfaces of,   the Work and Derivative Works thereof.</p> <p>\"Contribution\" shall mean any work of authorship, including   the original version of the Work and any modifications or additions   to that Work or Derivative Works thereof, that is intentionally   submitted to Licensor for inclusion in the Work by the copyright owner   or by an individual or Legal Entity authorized to submit on behalf of   the copyright owner. For the purposes of this definition, \"submitted\"   means any form of electronic, verbal, or written communication sent   to the Licensor or its representatives, including but not limited to   communication on electronic mailing lists, source code control systems,   and issue tracking systems that are managed by, or on behalf of, the   Licensor for the purpose of discussing and improving the Work, but   excluding communication that is conspicuously marked or otherwise   designated in writing by the copyright owner as \"Not a Contribution.\"</p> <p>\"Contributor\" shall mean Licensor and any individual or Legal Entity   on behalf of whom a Contribution has been received by Licensor and   subsequently incorporated within the Work.</p> </li> <li> <p>Grant of Copyright License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       copyright license to reproduce, prepare Derivative Works of,       publicly display, publicly perform, sublicense, and distribute the       Work and such Derivative Works in Source or Object form.</p> </li> <li> <p>Grant of Patent License. Subject to the terms and conditions of       this License, each Contributor hereby grants to You a perpetual,       worldwide, non-exclusive, no-charge, royalty-free, irrevocable       (except as stated in this section) patent license to make, have made,       use, offer to sell, sell, import, and otherwise transfer the Work,       where such license applies only to those patent claims licensable       by such Contributor that are necessarily infringed by their       Contribution(s) alone or by combination of their Contribution(s)       with the Work to which such Contribution(s) was submitted. If You       institute patent litigation against any entity (including a       cross-claim or counterclaim in a lawsuit) alleging that the Work       or a Contribution incorporated within the Work constitutes direct       or contributory patent infringement, then any patent licenses       granted to You under this License for that Work shall terminate       as of the date such litigation is filed.</p> </li> <li> <p>Redistribution. You may reproduce and distribute copies of the       Work or Derivative Works thereof in any medium, with or without       modifications, and in Source or Object form, provided that You       meet the following conditions:</p> <p>(a) You must give any other recipients of the Work or       Derivative Works a copy of this License; and</p> <p>(b) You must cause any modified files to carry prominent notices       stating that You changed the files; and</p> <p>(c) You must retain, in the Source form of any Derivative Works       that You distribute, all copyright, patent, trademark, and       attribution notices from the Source form of the Work,       excluding those notices that do not pertain to any part of       the Derivative Works; and</p> <p>(d) If the Work includes a \"NOTICE\" text file as part of its       distribution, then any Derivative Works that You distribute must       include a readable copy of the attribution notices contained       within such NOTICE file, excluding those notices that do not       pertain to any part of the Derivative Works, in at least one       of the following places: within a NOTICE text file distributed       as part of the Derivative Works; within the Source form or       documentation, if provided along with the Derivative Works; or,       within a display generated by the Derivative Works, if and       wherever such third-party notices normally appear. The contents       of the NOTICE file are for informational purposes only and       do not modify the License. You may add Your own attribution       notices within Derivative Works that You distribute, alongside       or as an addendum to the NOTICE text from the Work, provided       that such additional attribution notices cannot be construed       as modifying the License.</p> <p>You may add Your own copyright statement to Your modifications and   may provide additional or different license terms and conditions   for use, reproduction, or distribution of Your modifications, or   for any such Derivative Works as a whole, provided Your use,   reproduction, and distribution of the Work otherwise complies with   the conditions stated in this License.</p> </li> <li> <p>Submission of Contributions. Unless You explicitly state otherwise,       any Contribution intentionally submitted for inclusion in the Work       by You to the Licensor shall be under the terms and conditions of       this License, without any additional terms or conditions.       Notwithstanding the above, nothing herein shall supersede or modify       the terms of any separate license agreement you may have executed       with Licensor regarding such Contributions.</p> </li> <li> <p>Trademarks. This License does not grant permission to use the trade       names, trademarks, service marks, or product names of the Licensor,       except as required for reasonable and customary use in describing the       origin of the Work and reproducing the content of the NOTICE file.</p> </li> <li> <p>Disclaimer of Warranty. Unless required by applicable law or       agreed to in writing, Licensor provides the Work (and each       Contributor provides its Contributions) on an \"AS IS\" BASIS,       WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or       implied, including, without limitation, any warranties or conditions       of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A       PARTICULAR PURPOSE. You are solely responsible for determining the       appropriateness of using or redistributing the Work and assume any       risks associated with Your exercise of permissions under this License.</p> </li> <li> <p>Limitation of Liability. In no event and under no legal theory,       whether in tort (including negligence), contract, or otherwise,       unless required by applicable law (such as deliberate and grossly       negligent acts) or agreed to in writing, shall any Contributor be       liable to You for damages, including any direct, indirect, special,       incidental, or consequential damages of any character arising as a       result of this License or out of the use or inability to use the       Work (including but not limited to damages for loss of goodwill,       work stoppage, computer failure or malfunction, or any and all       other commercial damages or losses), even if such Contributor       has been advised of the possibility of such damages.</p> </li> <li> <p>Accepting Warranty or Additional Liability. While redistributing       the Work or Derivative Works thereof, You may choose to offer,       and charge a fee for, acceptance of support, warranty, indemnity,       or other liability obligations and/or rights consistent with this       License. However, in accepting such obligations, You may act only       on Your own behalf and on Your sole responsibility, not on behalf       of any other Contributor, and only if You agree to indemnify,       defend, and hold each Contributor harmless for any liability       incurred by, or claims asserted against, such Contributor by reason       of your accepting any such warranty or additional liability.</p> </li> </ol> <p>END OF TERMS AND CONDITIONS</p> <p>APPENDIX: How to apply the Apache License to your work.</p> <pre><code>  To apply the Apache License to your work, attach the following\n  boilerplate notice, with the fields enclosed by brackets \"[]\"\n  replaced with your own identifying information. (Don't include\n  the brackets!)  The text should be enclosed in the appropriate\n  comment syntax for the file format. We also recommend that a\n  file or class name and description of purpose be included on the\n  same \"printed page\" as the copyright notice for easier\n  identification within third-party archives.\n</code></pre> <p>Copyright 2024 Carlos Aumente Maestro</p> <p>Licensed under the Apache License, Version 2.0 (the \"License\");    you may not use this file except in compliance with the License.    You may obtain a copy of the License at</p> <pre><code>   http://www.apache.org/licenses/LICENSE-2.0\n</code></pre> <p>Unless required by applicable law or agreed to in writing, software    distributed under the License is distributed on an \"AS IS\" BASIS,    WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.    See the License for the specific language governing permissions and    limitations under the License.</p>"},{"location":"about/release-notes/","title":"Release Notes","text":""},{"location":"app/APP/","title":"Home page","text":""},{"location":"app/APP/#home-page","title":"Home page","text":""},{"location":"app/multivariate/","title":"Multivariate Analysis","text":""},{"location":"app/univariate/","title":"Univariate Analysis","text":""},{"location":"tutorials/postprocess_outputs/","title":"Post-processing model predictions with AUDIT","text":"<p>In this tutorial we will see how users can clean, organize and prepare their model's outputs for using AUDIT to  analyze the results.</p> <p>Let's assume we've selected a pre-trained model from an open-source repository to generate predictions on your  dataset. It is likely that the model wasn\u2019t trained to predict the same labels that our segmentations use. In such  cases, we\u2019ll need to apply some post-processing to properly evaluate these predictions using AUDIT.</p> <p>Fortunately, AUDIT provides users with tools to perform this post-processing and adapt the predictions as needed.  Let\u2019s walk through how to do this.</p>"},{"location":"tutorials/postprocess_outputs/#1-load-functions","title":"1. Load functions","text":"<p>This tutorial uses some utility functions from the <code>src.utils.operations.file_operations</code> and <code>src.utils.sequences</code>  modules to manipulate files and sequences.</p> <pre><code>import os\nfrom src.utils.operations.file_operations import (\n    ls_dirs, \n    ls_files, \n    rename_files, \n    rename_directories, \n    delete_files_by_extension, \n    organize_files_into_folders, \n    add_suffix_to_files\n)\nfrom src.utils.sequences import (\n    read_sequences_dict, \n    iterative_labels_replacement, \n    load_nii, \n    load_nii_by_id, \n    count_labels\n)\n</code></pre>"},{"location":"tutorials/postprocess_outputs/#2-data-understanding","title":"2. Data understanding","text":"<p>Now, let's suppose we have a model that was trained on a brain MRI dataset that we don't know in advance (in this case, it was BraTS2020). We use this model to generate a series of predictions on our own dataset. In our scenario, we want to run inference on BraTS2024 Pediatrics.</p> <p>After running the inference, we store the predictions in the following directory:</p> <pre><code># Define paths and folder names\nroot_path = \"/home/user/AUDIT/datasets/BraTS2024_PED/BraTS2024_PED_seg/\"\n</code></pre> <p>Let's check what is stored in the specified directory:</p> <pre><code>ls_files(root_path)[:6]\n</code></pre> <pre><code>['BraTS2024_PED-00001-000.nii.gz', \n'BraTS2024_PED-00001-000_ground_truth.nii.gz', \n'BraTS2024_PED-00001-000_segmentation.nii.gz', \n'BraTS2024_PED-00001-000_sequences.npy', \n'BraTS2024_PED-00002-000.nii.gz',\n 'BraTS2024_PED-00002-000_ground_truth.nii.gz']\n</code></pre> <p>As we can see, for some reason, the authors who designed this model not only stored the predictions but also saved  additional files.</p> <pre><code>file_1 = load_nii(path_folder=os.path.join(root_path, 'BraTS2024_PED-00001-000.nii.gz'), as_array=True)\nfile_2 = load_nii(path_folder=os.path.join(root_path, 'BraTS2024_PED-00001-000_segmentation.nii.gz'), as_array=True)\nfile_3 = load_nii(path_folder=os.path.join(root_path, 'BraTS2024_PED-00001-000_ground_truth.nii.gz'), as_array=True)\n\nprint(file_1.shape)\nprint(file_2.shape)\nprint(file_3.shape)\n</code></pre> <pre><code>(155, 240, 240)\n(160, 224, 160)\n(160, 224, 160)\n</code></pre>"},{"location":"tutorials/postprocess_outputs/#3-remove-unnecessary-data","title":"3. Remove unnecessary data","text":"<p>In this case, the developers of the model conducted other experiments during inference by modifying the original  dimensions of the images (155, 240, 240). As a result, all the files ending with \"_ground_truth,\" \"segmentation,\" and  \"_sequences\" are unnecessary for our purposes and should be deleted.</p> <p>To accomplish this, we can utilize a function from AUDIT called delete_files_by_extension. This function allows us to  repeatedly delete files from a directory based on their file extension.</p> <p>Here\u2019s how to use it:</p> <pre><code>delete_files_by_extension(root_dir=root_path, ext=\"npy\", verbose=True)\n</code></pre> <pre><code>Deleted file: /home/user/AUDIT/datasets/BraTS2024_PED/BraTS2024_PED_seg/BraTS2024_PED-00088-000_sequences.npy\nDeleted file: /home/user/AUDIT/datasets/BraTS2024_PED/BraTS2024_PED_seg/BraTS2024_PED-00141-000_sequences.npy\nDeleted file: /home/user/AUDIT/datasets/BraTS2024_PED/BraTS2024_PED_seg/BraTS2024_PED-00239-000_sequences.npy\n...\n...\n</code></pre> <p>We can confirm that all files with the specified extensions have been successfully deleted from the directory.  <pre><code>ls_files(root_path)[:6]\n</code></pre></p> <pre><code>['BraTS2024_PED-00001-000.nii.gz', \n'BraTS2024_PED-00001-000_ground_truth.nii.gz', \n'BraTS2024_PED-00001-000_segmentation.nii.gz', \n'BraTS2024_PED-00002-000.nii.gz',\n'BraTS2024_PED-00002-000_ground_truth.nii.gz'\n'BraTS2024_PED-00002-000_segmentation.nii.gz']\n</code></pre> <p>Now, let's proceed to remove the rest of the unnecessary files as well. We will apply the same  delete_files_by_extension function to eliminate any remaining files that we do not need.</p>"},{"location":"tutorials/postprocess_outputs/#4-organize-folder","title":"4. Organize folder","text":"<p>AUDIT is designed to work with multiple models and datasets, but it requires a specific organization of the data for  processing. Currently, we have a single directory \"/home/user/AUDIT/datasets/BraTS2024_PED/BraTS2024_PED_seg/\" </p> <p>This directory contains the predictions generated by a computer vision model for the BraTS2024 Pediatrics dataset.  However, AUDIT requires that each segmentation be contained within a folder named after the subject ID. Since users may not structure their code to produce output in this specific format\u2014like the pre-trained model we are  using\u2014AUDIT provides functions to facilitate the organization of files.</p> <p>In this case, we can use the organize_files_into_folders function to create the necessary data structure without  needing to perform complex manipulations.</p> <pre><code>organize_files_into_folders(root_path, extension='.nii.gz', verbose=True)\n\nprint(ls_files(root_path)[:6])\nprint(ls_dirs(root_path)[:6])\n</code></pre> <pre><code>[]\n\n['BraTS2024_PED-00001-000',\n 'BraTS2024_PED-00002-000',\n 'BraTS2024_PED-00003-000',\n 'BraTS2024_PED-00004-000',\n 'BraTS2024_PED-00005-000'\n 'BraTS2024_PED-00006-000']\n</code></pre> <p>As can be seen, the structure of the directory containing the predictions has changed. Instead of having all the files  scattered within the directory, a separate folder has been created for each subject, containing their respective  predictions. </p> <p>This organized structure makes it much easier to manage and process the predictions for each subject in the dataset.  Let\u2019s take a moment to verify this new organization:</p> <pre><code>|--BraTS2024_PED-00001-000\n|---- BraTS2024_PED-00001-000.nii.gz\n|--BraTS2024_PED-00002-000\n|---- BraTS2024_PED-00002-000.nii.gz\n...\n|--BraTS2024_PED-00266-000\n|---- BraTS2024_PED-00266-000.nii.gz\n</code></pre>"},{"location":"tutorials/postprocess_outputs/#5-add-extension-name","title":"5. Add extension name","text":"<p>Another important aspect of AUDIT is that it uses file extensions to distinguish between sequences (e.g., _t1, _t2,  _t1ce, _flair), segmentations (_seg), and predictions (_pred`). However, if we recall the names of the files  generated after inference, they did not contain any specific nomenclature (i.e. BraTS2024_PED-00001-000.nii.gz).</p> <p>To simplify the task for users and eliminate the need to modify their pipelines to ensure compatibility with AUDIT,  the library provides methods to quickly and easily adapt the file names. </p> <p>Let\u2019s explore how to rename the files so they align with AUDIT\u2019s requirements. Taking advantage of the function  add_suffix_to_files, we can add the extension requered by AUDIT.</p> <pre><code>add_suffix_to_files(folder_path=root_path, suffix='_pred', ext='.nii.gz', verbose=True)\n</code></pre> <p>Now, all the files within the root_path contain the _pred extension.</p> <pre><code>ls_files(os.path.join(root_path, 'BraTS2024_PED-00001-000'))\n</code></pre> <pre><code>['BraTS2024_PED-00001-000_pred.nii.gz']\n</code></pre>"},{"location":"tutorials/postprocess_outputs/#6-label-replacement","title":"6. Label replacement","text":"<p>Last but not least, we need to verify which labels were generated after the inference. As mentioned, the model used  was pre-trained on the BraTS2020 dataset to predict the labels ENH, NEC, and EDE. However, these may not align with the  labels used in the BraTS2024 Pediatrics dataset. In fact, they are different and will need to be adjusted accordingly.</p> <p>Let\u2019s check the generated labels and discuss how to adapt them to match the required labels for the BraTS2024  Pediatrics dataset.</p> <pre><code>pred = load_nii_by_id(root=root_path, patient_id='BraTS2024_PED-00001-000', seq=\"_pred\", as_array=True)\ncount_labels(pred)\n</code></pre> <pre><code>{0.0: 7664276, 1.0: 106230, 2.0: 325915, 4.0: 831579}\n</code></pre> <p>As we discussed earlier, the labels used in BraTS2020 are [0, 4, 1, 2] for the labels ENH, NEC, and EDE, respectively.  In contrast, the labels for BraTS2024 Pediatrics are [0, 1, 2, 3] for the same categories.</p> <p>To resolve this mismatch, we will use the iterative_labels_replacement function. This function takes the old mapping  and the new mapping as parameters and replaces the labels accordingly.</p> <pre><code># old mappings\nbrats2020_mapping = [0, 4, 1, 2]\n# new mapping\nnew_mapping = [0, 1, 2, 3]\n\niterative_labels_replacement(\n    root_dir=root_path, \n    original_labels=brats2020_mapping, \n    new_labels=new_mapping,\n    ext='pred',\n    verbose=True\n)\n</code></pre> <p>Once we apply this function, we can confirm that the labels are now correct.</p> <pre><code>pred = load_nii_by_id(root=root_path, patient_id='BraTS2024_PED-00001-000', seq=\"_pred\", as_array=True)\ncount_labels(pred)\n</code></pre> <pre><code>{0.0: 7664276, 1.0: 831579, 2.0: 106230, 3.0: 325915}\n</code></pre> <p>Additionally, by comparing the labels from the predictions with those from the original segmentation, we can see that  both use the range from 0 to 3.</p> <pre><code>seg = load_nii_by_id(\n    root=\"/home/carlos/Documentos/proyectos/AUDIT/datasets/BraTS2024_PED/BraTS2024_PED_images/\", \n    patient_id='BraTS2024_PED-00001-000', \n    seq=\"_seg\", \n    as_array=True\n)\n\ncount_labels(seg)\n</code></pre> <pre><code>{0: 8799333, 1: 6204, 2: 87918, 3: 34545}\n</code></pre> <p>Finally, we have prepared the dataset to run the metric_extractor.py module and start using AUDIT effectively.</p>"},{"location":"tutorials/tutorials/","title":"Tutorials","text":"<p>Here are provided some examples about the use of AUDIT</p>"},{"location":"tutorials/tutorials/#data-manipulation","title":"Data Manipulation","text":""},{"location":"tutorials/tutorials/#feature-extraction","title":"Feature extraction","text":""},{"location":"tutorials/tutorials/#metric-extraction","title":"Metric extraction","text":""},{"location":"user-guide/features/feature_extractor/","title":"Feature extractor","text":"<p>This <code>feature extraction</code> pipeline is designed to process medical imaging datasets, specifically MRI scans, to extract a  wide range of features including spatial, tumor-related, statistical, and texture-based characteristics. The pipeline  is composed of two core components: the feature_extractor.py script, which orchestrates the entire process, and the  underlying feature extraction logic contained in src.features.main.py.</p> <p>The pipeline operates as follows:</p> <ul> <li> <p>Configuration and Logging: The process begins by loading configuration settings from a YAML file, specifying data  paths, features to extract, and output directories. A logging system is set up to monitor and record the progress of  the feature extraction.</p> </li> <li> <p>Dataset Processing: For each dataset defined in the configuration, the pipeline iterates over all patient data,  extracting features from MRI sequences and associated segmentations. This is handled by the extract_features function, which computes various features such as spatial dimensions, tumor characteristics, statistical metrics, and texture  properties of the images.</p> </li> <li> <p>Feature Extraction: The pipeline uses specialized classes for different types of features:</p> <ul> <li>Spatial Features: Related to image dimensions and brain structure.</li> <li>Tumor Features: Derived from segmentations to describe the tumor\u2019s shape, volume, and position.</li> <li>Statistical Features: First-order statistics like mean, variance, etc., extracted from image sequences.</li> <li>Texture Features: Second-order metrics describing the texture patterns in the images.</li> </ul> </li> </ul> <p>If longitudinal data is present, the pipeline also extracts and includes time-point and longitudinal  identifiers for further analysis.</p> <ul> <li>Data Output: Once features are extracted for each patient, they are compiled into a DataFrame, which is saved as  a CSV file. </li> </ul> <p>This pipeline provides an automated and extensible framework for processing large-scale MRI datasets, ensuring that  all relevant features are extracted and saved for downstream analysis, such as predictive modeling or visualization.</p> <p>In the following sections, the users can explore in detail the different methods provided by AUDIT to extract relevant  information from MRIs.</p>"},{"location":"user-guide/features/spatial/","title":"Spatial features","text":"<p>The <code>SpatialFeatures</code> class is designed to compute spatial properties related to 3D medical imaging sequences, such as  brain MRI scans. This class focuses on calculating basic spatial features like the brain's center of mass and the  dimensionality of the scan in various planes.</p>"},{"location":"user-guide/features/spatial/#overview","title":"Overview","text":"<p>This class is intended to provide spatial insights from a 3D sequence of medical images. It helps to extract two key  metrics: The center of mass for the brain, which is calculated based on the sequence. The dimensions of the sequence in the axial, coronal, and sagittal planes. These spatial features are essential in understanding the brain's structure  and alignment in a 3D scan, aiding in medical analysis and further processing of brain images.</p> <p>The following spatial features are available:</p> <ul> <li>Brain Center of Mass: The 3D coordinates of the brain's center, adjusted by voxel spacing.</li> <li>Sequence Dimensions: The dimensions of the sequence in the axial, coronal, and sagittal planes.</li> </ul>"},{"location":"user-guide/features/spatial/#methods","title":"Methods","text":""},{"location":"user-guide/features/spatial/#__init__","title":"<code>__init__()</code>","text":"<p>Description: The constructor method initializes a <code>SpatialFeatures</code> object with a given 3D image sequence and voxel spacing. The  voxel spacing is used for any adjustments to the spatial features (e.g., center of mass). If no spacing is provided,  it defaults to (1, 1, 1).</p> <p>Parameters:</p> <ul> <li><code>sequence</code> (<code>np.ndarray</code>): A 3D MRI in form of NumPy array from which spatial features will be calculated.</li> <li><code>spacing</code> (<code>np.ndarray, optional</code>): A tuple representing the voxel spacing of the medical image. Defaults to  (1, 1, 1) if not provided.</li> </ul>"},{"location":"user-guide/features/spatial/#calculate_brain_center_mass","title":"<code>calculate_brain_center_mass()</code>","text":"<p>Returns (<code>dict</code>): A dictionary containing the 3D coordinates of the brain's center of mass for each plane (axial,  coronal, sagittal), adjusted by the voxel spacing. If the sequence is not found, it returns NaN for each plane.</p>"},{"location":"user-guide/features/spatial/#get_dimensions","title":"<code>get_dimensions()</code>","text":"<p>Returns (<code>dict</code>): A dictionary containing the dimensions of the sequence for each plane (axial_dim, coronal_dim, and  sagittal_dim). If the sequence is not found, the dimensions are returned as NaN.</p>"},{"location":"user-guide/features/spatial/#extract_features","title":"<code>extract_features()</code>","text":"<p>Returns (<code>dict</code>): All spatial features. </p> <ul> <li><code>axial_dim</code>: Number of slices in the axial plane.</li> <li><code>coronal_dim</code>: Number of slices in the coronal plane.</li> <li><code>sagittal_dim</code>: Number of slices in the sagittal plane.</li> <li><code>axial_brain_centre_mass</code>: The center of mass in the axial plane.</li> <li><code>coronal_brain_centre_mass</code>: The center of mass in the coronal plane.</li> <li><code>sagittal_brain_centre_mass</code>: The center of mass in the sagittal plane.</li> </ul>"},{"location":"user-guide/features/statistical/","title":"Statistical features","text":"<p>The <code>StatisticalFeatures</code> class provides a convenient way to compute several common statistical metrics from a given array of data.</p>"},{"location":"user-guide/features/statistical/#overview","title":"Overview","text":"<p>The class utilizes NumPy for efficient numerical operations and SciPy for computing first-order statistical features. By encapsulating these features in a class, users can easily compute various statistical properties of a dataset with minimal boilerplate code.</p> <p>The following statistical features are available:</p> <ul> <li>Maximum intensity: The highest value in the MRI.</li> <li>Minimum intensity: The lowest value in the MRI.</li> <li>Mean intensity: The average value of the MRI.</li> <li>Median intensity: The middle value of the MRI when sorted.</li> <li>Standard deviation intensity: A measure of the amount of variation or dispersion of the values.</li> <li>Range intensity: The difference between the maximum and minimum values.</li> <li>Skewness: A measure of the asymmetry of the distribution of pixel values.</li> </ul>"},{"location":"user-guide/features/statistical/#methods","title":"Methods","text":""},{"location":"user-guide/features/statistical/#__init__","title":"<code>__init__()</code>","text":"<p>Description: The constructor method initializes the <code>StatisticalFeatures</code> object by accepting a 3D magnetic resonance image in the form of a NumPy array. Once initialized, the class provides several methods to compute various statistical features from the MRI. Notice that, given the background of MRI images usually is equal to 0, those values should be removed to not interfere with the computations.</p> <p>Parameters:</p> <ul> <li><code>sequence</code> (<code>np.ndarray</code>): A 3D MRI in form of NumPy array from which statistical features will be calculated.</li> </ul>"},{"location":"user-guide/features/statistical/#get_max_intensity","title":"<code>get_max_intensity()</code>","text":"<p>Returns (<code>float</code>): The maximum intensity value in the MRI.</p>"},{"location":"user-guide/features/statistical/#get_min_intensity","title":"<code>get_min_intensity()</code>","text":"<p>Returns (<code>float</code>): The minimum intensity value in the MRI.</p>"},{"location":"user-guide/features/statistical/#get_mean_intensity","title":"<code>get_mean_intensity()</code>","text":"<p>Returns (<code>float</code>): The mean intensity value in the MRI.  </p>"},{"location":"user-guide/features/statistical/#get_median_intensity","title":"<code>get_median_intensity()</code>","text":"<p>Returns (<code>float</code>): The median intensity value in the MRI.</p>"},{"location":"user-guide/features/statistical/#get_std_intensity","title":"<code>get_std_intensity()</code>","text":"<p>Returns (<code>float</code>): The standard deviation of the of intensity values in the MRI.</p>"},{"location":"user-guide/features/statistical/#get_range_intensity","title":"<code>get_range_intensity()</code>","text":"<p>Returns (<code>float</code>): the range of intensity values in the MRI (i.e., the difference between the maximum and minimum values).</p>"},{"location":"user-guide/features/statistical/#get_skewness","title":"<code>get_skewness()</code>","text":"<p>Returns (<code>float</code>): The skewness of the intensity values in the MRI, a measure of asymmetry in the distribution.</p>"},{"location":"user-guide/features/statistical/#extract_features","title":"<code>extract_features()</code>","text":"<p>Returns (<code>dict</code>): All statistical features. </p> <ul> <li><code>max_intensity</code>: Maximum intensity value in the MRI.</li> <li><code>min_intensity</code>: Minimum intensity value in the MRI.</li> <li><code>mean_intensity</code>: Mean intensity value in the MRI.</li> <li><code>median_intensity</code>: Median intensity value in the MRI.</li> <li><code>std_intensity</code>: Standard deviation of intensity values in the MRI.</li> <li><code>range_intensity</code>: Range of intensity values in the MRI.</li> <li><code>skewness</code>: Skewness of the intensity values in the MRI.</li> </ul>"},{"location":"user-guide/features/texture/","title":"Texture features","text":"<p>The <code>TextureFeatures</code> class provides an efficient mechanism for calculating second-order texture features from a given 3D magnetic resonance image (MRI).</p>"},{"location":"user-guide/features/texture/#overview","title":"Overview","text":"<p>This class utilizes skimage for calculating the gray level co-occurrence matrix (GLCM) and its corresponding texture  features such as contrast, homogeneity, and energy. The texture features extracted from each 2D plane of a 3D MRI  sequence give insights into the structural patterns within the image.</p> <p>By encapsulating these operations in a class, the user can easily compute several texture features with minimal effort. It also supports an option to remove empty planes, improving accuracy when working with brain MRI scans.</p> <p>The following texture features are available:</p> <ul> <li>Contrast: A measure of the intensity contrast between a pixel and its neighbor over the whole image.</li> <li>Dissimilarity: Measures the local intensity variations.</li> <li>Homogeneity: Measures the closeness of the distribution of elements in the GLCM to the GLCM diagonal.</li> <li>ASM (Angular Second Moment): A measure of the texture uniformity.</li> <li>Energy: The square root of ASM, indicating the texture\u2019s level of orderliness.</li> <li>Correlation: A measure of how correlated a pixel is to its neighbor across the whole image.</li> </ul>"},{"location":"user-guide/features/texture/#methods","title":"Methods","text":""},{"location":"user-guide/features/texture/#__init__","title":"<code>__init__()</code>","text":"<p>Description:</p> <p>The constructor initializes a <code>TextureFeatures</code> object by accepting a 3D MRI sequence and an optional parameter to  remove empty planes. The class will compute texture features across 2D planes, making use of GLCM-based calculations.</p> <p>Parameters:</p> <ul> <li>sequence (np.ndarray): A 3D MRI image in the form of a NumPy array from which texture features will be calculated.</li> <li>remove_empty_planes (bool): A flag to indicate whether empty planes (e.g., non-brain areas) should be removed from the MRI sequence. Defaults to False. </li> </ul>"},{"location":"user-guide/features/texture/#compute_texture_values","title":"<code>compute_texture_values()</code>","text":"<p>Description: Computes the specified texture feature for each 2D plane in the 3D image. The GLCM is calculated for each 2D slice, and the texture feature is extracted using graycoprops from the skimage.feature module.</p> <p>Parameters:</p> <ul> <li><code>texture</code> (<code>str</code>): The texture feature to compute (e.g., \"contrast\", \"homogeneity\"). Defaults to \"contrast\".</li> </ul> <p>Returns (<code>np.ndarray</code>): An array of texture values for each 2D plane in the 3D MRI sequence.</p>"},{"location":"user-guide/features/texture/#extract_features","title":"<code>extract_features()</code>","text":"<p>Description:</p> <p>Extracts all specified texture features from the MRI image. This method iterates through the given list of texture features, computing the mean and standard deviation for each one across all 2D planes in the MRI sequence.</p> <p>Parameters:</p> <ul> <li><code>textures</code> (`list[str]): A list of texture features to compute (e.g., 'contrast', 'energy'). If not provided, the      default set includes: 'contrast', 'dissimilarity', 'homogeneity', 'ASM', 'energy', 'correlation'</li> </ul> <p>Returns (<code>dict</code>): A dictionary where the keys represent the texture feature names, and the values represent the mean  and standard deviation for each feature.</p>"},{"location":"user-guide/features/tumor/","title":"Tumor features","text":"<p>The <code>TumorFeatures</code> class computes various tumor-related metrics based on a segmented 3D medical image, such as lesion  size, tumor center of mass, and tumor location relative to the brain's center of mass.</p>"},{"location":"user-guide/features/tumor/#overview","title":"Overview","text":"<p>This class provides methods to compute first-order tumor features, focusing on spatial and volumetric characteristics  derived from medical image segmentation. It is designed to handle common use cases in medical imaging, such as  identifying tumor regions, calculating the tumor's size, and determining its relative position.</p> <p>The class allows customization through optional parameters such as voxel spacing and segmentation label mappings. This  makes it highly adaptable to different medical imaging contexts, including various scan types and segmentation  algorithms.</p> <p>The following tumor features are available:</p> <ul> <li>Tumor Pixel Count: The number of pixels associated with each tumor label in the segmentation.</li> <li>Lesion Size: The volume of the lesion(s) computed based on pixel count and voxel spacing.</li> <li>Tumor Center of Mass: The geometric center of a tumor or lesion in 3D space.</li> <li>Tumor Slices: The image slices in the axial, coronal, and sagittal planes that contain tumor regions.</li> <li>Tumor Position: The location of tumor slices in each plane (e.g., lower and upper bounds).</li> </ul>"},{"location":"user-guide/features/tumor/#methods","title":"Methods","text":""},{"location":"user-guide/features/tumor/#__init__","title":"<code>__init__()</code>","text":"<p>Description: The constructor initializes a <code>TumorFeatures</code> object using a 3D segmented MRI and various optional parameters such as  voxel spacing and label mappings. These attributes are used to calculate tumor features like lesion size and tumor  location across different planes.</p> <p>Parameters:</p> <ul> <li><code>segmentation</code> (<code>np.ndarray</code>): A 3D NumPy array representing the segmentation of the medical image.</li> <li><code>spacing</code> (<code>tuple, optional</code>): The voxel spacing of the medical image (default is (1, 1, 1)).</li> <li><code>mapping_names</code> (<code>dict, optional</code>): A dictionary mapping segmentation values (labels) to human-readable names.</li> <li><code>planes</code> (<code>list[str]</code>, optional`): The planes (axial, coronal, sagittal) for tumor slice analysis. Defaults to  [\"axial\", \"coronal\", \"sagittal\"].</li> </ul>"},{"location":"user-guide/features/tumor/#count_tumor_pixels","title":"<code>count_tumor_pixels()</code>","text":"<p>Returns (<code>dict</code>):  A dictionary where keys represent segmentation labels (or names) and values represent the pixel  count for each label.</p>"},{"location":"user-guide/features/tumor/#count_tumor_pixels_1","title":"<code>count_tumor_pixels()</code>","text":"<p>Returns (<code>dict</code>): A dictionary containing the total lesion size, keyed by \"lesion_size\".</p>"},{"location":"user-guide/features/tumor/#get_tumor_center_masslabelnone","title":"<code>get_tumor_center_mass(label=None)</code>","text":"<p>Parameters:</p> <ul> <li><code>label</code> (<code>int, optional</code>): Specifies the label of the tumor for which the center of mass should be calculated. If not  provided, the calculation is done for all tumor regions.</li> </ul> <p>Return (<code>np.ndarray</code>): The 3D coordinates of the tumor\u2019s center of mass.</p>"},{"location":"user-guide/features/tumor/#get_tumor_slices","title":"<code>get_tumor_slices()</code>","text":"<p>Return (<code>tuple</code>): A tuple containing three lists, each representing the indices of slices with tumors in the axial,  coronal, and sagittal planes.</p>"},{"location":"user-guide/features/tumor/#calculate_tumor_slices","title":"<code>calculate_tumor_slices()</code>","text":"<p>Return (<code>dict</code>): A dictionary where keys represent the plane names (e.g., \"axial_tumor_slice\") and values represent the  number of tumor-containing slices.</p>"},{"location":"user-guide/features/tumor/#calculate_position_tumor_slices","title":"<code>calculate_position_tumor_slices()</code>","text":"<p>Return (<code>dict</code>): A dictionary containing the minimum and maximum slice indices for each plane  (e.g., \"lower_axial_tumor_slice\", \"upper_axial_tumor_slice\").</p>"},{"location":"user-guide/features/tumor/#calculate_tumor_pixel","title":"<code>calculate_tumor_pixel()</code>","text":"<p>Return (<code>dict</code>): A dictionary where keys represent each tumor label (e.g., \"lesion_size_label1\") and values represent  the lesion size in voxels.</p>"},{"location":"user-guide/features/tumor/#calculate_tumor_distance","title":"<code>calculate_tumor_distance()</code>","text":"<p>Parameters:</p> <ul> <li><code>brain_centre_mass</code> (<code>array-like</code>): The center of mass of the brain used as a reference point.</li> </ul> <p>Return (<code>dict</code>): A dictionary where each key represents the tumor label and the value represents the distance between  the tumor and the brain's center of mass.</p>"},{"location":"user-guide/features/tumor/#calculate_tumor_center_mass","title":"<code>calculate_tumor_center_mass()</code>","text":"<p>Parameters:</p> <p>Return (<code>dict</code>):  A dictionary where keys represent each plane and label (e.g., \"axial_tumor_center_mass\") and values  represent the coordinates of the tumor center of mass.</p>"},{"location":"user-guide/features/tumor/#extract_features","title":"<code>extract_features()</code>","text":"<p>Returns (<code>dict</code>): All tumor features. </p> <ul> <li>Center of mass per label and plane (e.g., \"axial_tumor_center_mass\").</li> <li>Tumor location relative to brain center of mass.</li> <li>Lesion size per label.</li> <li>Total lesion size.</li> <li>Number of tumor-containing slices per plane.</li> <li>Lower and upper bounds of tumor slices.</li> </ul>"},{"location":"user-guide/metrics/custom_metrics/","title":"Metrics Computed","text":"<p>The provided code computes a variety of metrics to evaluate the performance of a segmentation model in relation to the  ground truth. These metrics provide insights into the model's accuracy, overlap, and shape conformity with the actual  segmented regions. Below is an overview of each metric computed:</p>"},{"location":"user-guide/metrics/custom_metrics/#1-dice-score-dice","title":"1. Dice Score (DICE)","text":"<p>The Dice score (or Dice coefficient) is a measure of overlap between the ground truth and the predicted segmentation. It ranges from 0 to 1, with 1 indicating perfect overlap.</p> \\[ \\text{Dice} = \\frac{2 \\cdot TP}{2 \\cdot TP + FP + FN} \\] <p>Where <code>TP</code> is true positives, <code>FP</code> is false positives, and <code>FN</code> is false negatives.</p> <p>Interpretation: A higher Dice score indicates better agreement between prediction and ground truth.</p>"},{"location":"user-guide/metrics/custom_metrics/#2-jaccard-index-jacc","title":"2. Jaccard Index (JACC)","text":"<p>Also known as the Intersection over Union (IoU), the Jaccard index is another overlap-based metric. It measures the  size of the intersection divided by the size of the union of the predicted and ground truth regions.</p> \\[ \\text{Jaccard} = \\frac{TP}{TP + FP + FN} \\] <p>Interpretation: A higher Jaccard index indicates a more accurate segmentation. It is always lower than the Dice score  for the same segmentation.</p>"},{"location":"user-guide/metrics/custom_metrics/#3-sensitivity-sens","title":"3. Sensitivity (SENS)","text":"<p>Sensitivity, also known as recall or true positive rate, measures the ability of the model to correctly identify all  the positive regions (i.e., tumor voxels).</p> \\[ \\text{Sensitivity} = \\frac{TP}{TP + FN} \\] <p>Interpretation: A higher sensitivity value indicates the model is good at detecting positive regions (e.g., tumor  regions), but it doesn\u2019t account for false positives.</p>"},{"location":"user-guide/metrics/custom_metrics/#4-specificity-spec","title":"4. Specificity (SPEC)","text":"<p>Specificity measures the model's ability to correctly identify negative regions (i.e., non-tumor voxels).</p> \\[ \\text{Specificity} = \\frac{TN}{TN + FP} \\] <p>Interpretation: A higher specificity value indicates the model is good at ignoring false positives, but doesn\u2019t account for missing true positives.</p>"},{"location":"user-guide/metrics/custom_metrics/#5-precision-prec","title":"5. Precision (PREC)","text":"<p>Precision, or positive predictive value, measures the proportion of predicted positive cases that are actually positive.</p> \\[ \\text{Precision} = \\frac{TP}{TP + FP} \\] <p>Interpretation: A high precision value means that when the model predicts a positive case (e.g., tumor), it is likely  correct.</p>"},{"location":"user-guide/metrics/custom_metrics/#6-accuracy-accu","title":"6. Accuracy (ACCU)","text":"<p>Accuracy provides an overall measure of how often the model makes correct predictions (both true positives and true  negatives) across all regions.</p> \\[ \\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN} \\] <p>Interpretation: A high accuracy score reflects the model\u2019s general performance in predicting both positive and negative cases.</p>"},{"location":"user-guide/metrics/custom_metrics/#7-hausdorff-distance-haus","title":"7. Hausdorff Distance (HAUS)","text":"<p>The Hausdorff distance is a shape-based metric that measures the maximum distance between points on the predicted  segmentation and the corresponding points on the ground truth.</p> \\[ \\text{Hausdorff Distance} = \\max_{x \\in A} \\min_{y \\in B} d(x, y) \\] <p>Where <code>A</code> is the set of points on the predicted segmentation, <code>B</code> is the set of points on the ground truth, and  <code>d(x, y)</code> is the Euclidean distance between points.</p> <p>Interpretation: Lower Hausdorff distances indicate that the boundary of the predicted segmentation is closer to the ground truth boundary, implying better shape similarity.</p>"},{"location":"user-guide/metrics/custom_metrics/#8-segmentation-size-size","title":"8. Segmentation Size (SIZE)","text":"<p>This metric calculates the physical size of the predicted segmentation in terms of voxel count, adjusted by the voxel spacing to provide a volume measurement.</p> \\[ \\text{Size} = \\text{Voxel count of predicted region} \\times \\text{Spacing} \\] <p>Interpretation: This helps to quantify the total volume of the segmented region, which can be compared to the expected  size from the ground truth.</p>"},{"location":"user-guide/metrics/metric_extractor/","title":"Metric extractor","text":"<p>This <code>metric extraction</code> pipeline processes MRI segmentation data by comparing ground truth segmentations with model  predictions to compute a variety of metrics. The pipeline supports both custom metrics and Pymia-based metrics, with  the ability to handle multiple models and datasets. The two main components of the pipeline are metric_extractor.py,  which serves as the entry point, and main.py, which contains the core logic for metric computation.</p> <p>The pipeline operates as follows:</p> <ul> <li> <p>Configuration and Logging: The pipeline begins by loading a configuration file, which defines the dataset paths,  models, metrics to be extracted, and output settings. Logging is set up to track the progress and output detailed logs.</p> </li> <li> <p>Metric Extraction: Depending on the configuration, the pipeline can compute either custom metrics (using methods  defined in the project) or Pymia metrics (using the Pymia library for medical image analysis). </p> </li> <li> <p>Custom Metrics: This approach calculates specific metrics like Dice coefficient, sensitivity, or others based on    custom implementations. It involves one-hot encoding the ground truth and predicted segmentations and then computing    the defined metrics for each patient.</p> </li> <li> <p>Pymia Metrics: The pipeline can leverage Pymia's built-in metrics (e.g., Hausdorff distance, Dice coefficient,    Jaccard index) for segmentation evaluation. Pymia's evaluator processes the segmentation files and accumulates the    results across different models and regions.</p> </li> <li> <p>Data Processing: For each dataset and model, metrics are computed for all patients. The results are collected  into a DataFrame, and if longitudinal data is involved, it can further organize the results by time points.</p> </li> <li> <p>Output and Statistics: The extracted metrics are stored as CSV files, and additional statistical analyses (e.g.,  mean, median, standard deviation) can be computed and saved if required. The output is structured and ready for  further analysis or reporting.</p> </li> </ul> <p>This pipeline provides a flexible and scalable solution for evaluating segmentation models, making it suitable for  multi-model comparisons and performance tracking across different datasets.</p>"}]}